{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, month, min, max, avg\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/19 17:33:55 WARN Utils: Your hostname, LAPTOP-RELH58H1 resolves to a loopback address: 127.0.1.1; using 172.19.22.4 instead (on interface eth0)\n",
      "23/08/19 17:33:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/19 17:33:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/19 17:34:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.executor.memory', '2g')\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \n",
    "    .config('spark.executor.memoryOverhead', '3000')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Taxi Data (no shooting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define a list of 3-month segments\n",
    "segments = [\n",
    "    \"2016_01_to_03\", \"2016_04_to_06\", \"2016_07_to_09\", \"2016_10_to_12\",\n",
    "    \"2017_01_to_03\", \"2017_04_to_06\", \"2017_07_to_09\", \"2017_10_to_12\",\n",
    "    \"2018_01_to_03\", \"2018_04_to_06\", \"2018_07_to_09\", \"2018_10_to_12\",\n",
    "    \"2019_01_to_03\", \"2019_04_to_06\", \"2019_07_to_09\", \"2019_10_to_12\"\n",
    "]\n",
    "\n",
    "# Create a list to store the DataFrames\n",
    "taxi_periods = []\n",
    "\n",
    "# Create DataFrames for each segment and add to the list\n",
    "for segment in segments:\n",
    "    segment_df = spark.read.parquet(f'../data/raw/yellow_taxi_hourly/{segment}.parquet/*')\n",
    "    taxi_periods.append(segment_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Shooting Trip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of 3-month segments\n",
    "segments = [\n",
    "    \"2016_01_to_03\", \"2016_04_to_06\", \"2016_07_to_09\", \"2016_10_to_12\",\n",
    "    \"2017_01_to_03\", \"2017_04_to_06\", \"2017_07_to_09\", \"2017_10_to_12\",\n",
    "    \"2018_01_to_03\", \"2018_04_to_06\", \"2018_07_to_09\", \"2018_10_to_12\",\n",
    "    \"2019_01_to_03\", \"2019_04_to_06\", \"2019_07_to_09\", \"2019_10_to_12\"\n",
    "]\n",
    "\n",
    "# Create a list to store the DataFrames\n",
    "shooting_trips = []\n",
    "\n",
    "# Create DataFrames for each segment and add to the list\n",
    "for segment in segments:\n",
    "    segment_df = spark.read.parquet(f'../data/curated/shooting_taxi_hour_data_reduced/{segment}.parquet/*')\n",
    "    shooting_trips.append(segment_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Month column for shooting data and rename column as Hour (of pickup) - wasn't able to save other dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store modified DataFrames with the \"month\" and \"hour\" columns\n",
    "timed_shooting_periods = []\n",
    "\n",
    "# Loop through the list of DataFrames and add the \"month\" and \"hour\" columns\n",
    "for taxi_df in shooting_trips:\n",
    "    full_modified_taxi_df = taxi_df.withColumn(\"Month\", month(col(\"Date\"))).withColumnRenamed(\"pickup_hour\", \"Hour\")\n",
    "    timed_shooting_periods.append(full_modified_taxi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename PULocationID as LocationID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store modified DataFrames with the \"month\" and \"hour\" columns\n",
    "timed_periods = []\n",
    "\n",
    "# Loop through the list of DataFrames and add the \"month\" and \"hour\" columns\n",
    "for taxi_df in taxi_periods:\n",
    "    full_modified_taxi_df = taxi_df.withColumnRenamed(\"PULocationID\", \"LocationID\")\n",
    "    timed_periods.append(full_modified_taxi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join into one big dataframe - First Taxi, then shooting trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all DataFrames in the taxi_periods list using union\n",
    "combined_taxi_dataframe = reduce(DataFrame.unionAll, timed_periods)\n",
    "combined_shooting_dataframe = reduce(DataFrame.unionAll, timed_shooting_periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to count size of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_rows(dataframe):\n",
    "    \"\"\"\n",
    "    Calculates the total number of rows in DataFrame and prints the result\n",
    "    Inputs:\n",
    "        DataFrame for which to calculate the total rows\n",
    "    \"\"\"\n",
    "    total_rows = dataframe.count()\n",
    "    print(\"Total rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count size of shooting aggregated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_total_rows(combined_shooting_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count size of taxi aggregated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_total_rows(combined_taxi_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier Detection - Remove Outliers from number of trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:============================>                            (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of number_of_trips values (shooting): [1, 3111]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the minimum and maximum values for the 'number_of_trips' column for shooting trips\n",
    "min_value = combined_shooting_dataframe.select(min(\"number_of_trips\")).first()[0]\n",
    "max_value = combined_shooting_dataframe.select(max(\"number_of_trips\")).first()[0]\n",
    "\n",
    "# Print or display the range of values\n",
    "print(f\"Range of number_of_trips values (shooting): [{min_value}, {max_value}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:=================================================>      (63 + 8) / 72]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of number_of_trips values (taxi): [1, 3746]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the minimum and maximum values for the 'number_of_trips' column for taxi trips\n",
    "min_value = combined_taxi_dataframe.select(min(\"number_of_trips\")).first()[0]\n",
    "max_value = combined_taxi_dataframe.select(max(\"number_of_trips\")).first()[0]\n",
    "\n",
    "# Print or display the range of values\n",
    "print(f\"Range of number_of_trips values (taxi): [{min_value}, {max_value}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate mean and standard deviation\n",
    "mean_std_df = combined_shooting_dataframe.selectExpr(\"avg(number_of_trips) as mean\", \"stddev(number_of_trips) as std_dev\")\n",
    "mean_std = mean_std_df.first()\n",
    "\n",
    "# Calculate Z-score\n",
    "z_score_df = combined_shooting_dataframe.withColumn(\"z_score\", (col(\"number_of_trips\") - mean_std.mean) / mean_std.std_dev)\n",
    "\n",
    "# Set the Z-score threshold for removing outliers\n",
    "z_score_threshold = 3.0\n",
    "\n",
    "# Filter out rows with Z-scores above the threshold\n",
    "remove_outliers_shooting_dataframe = z_score_df.filter(col(\"z_score\") <= z_score_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate mean and standard deviation\n",
    "mean_std_df = combined_taxi_dataframe.selectExpr(\"avg(number_of_trips) as mean\", \"stddev(number_of_trips) as std_dev\")\n",
    "mean_std = mean_std_df.first()\n",
    "\n",
    "# Calculate Z-score\n",
    "z_score_df = combined_taxi_dataframe.withColumn(\"z_score\", (col(\"number_of_trips\") - mean_std.mean) / mean_std.std_dev)\n",
    "\n",
    "# Set the Z-score threshold for removing outliers\n",
    "z_score_threshold = 3.0\n",
    "\n",
    "# Filter out rows with Z-scores above the threshold\n",
    "remove_outliers_taxi_dataframe = z_score_df.filter(col(\"z_score\") <= z_score_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:============================>                            (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of number_of_trips values (shooting): [1, 175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the minimum and maximum values for the 'number_of_trips' column\n",
    "min_value = remove_outliers_shooting_dataframe.select(min(\"number_of_trips\")).first()[0]\n",
    "max_value = remove_outliers_shooting_dataframe.select(max(\"number_of_trips\")).first()[0]\n",
    "\n",
    "# Print or display the range of values\n",
    "print(f\"Range of number_of_trips values (shooting): [{min_value}, {max_value}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:=======================================================>(71 + 1) / 72]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of number_of_trips values (taxi): [1, 610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the minimum and maximum values for the 'number_of_trips' column\n",
    "min_value = remove_outliers_taxi_dataframe.select(min(\"number_of_trips\")).first()[0]\n",
    "max_value = remove_outliers_taxi_dataframe.select(max(\"number_of_trips\")).first()[0]\n",
    "\n",
    "# Print or display the range of values\n",
    "print(f\"Range of number_of_trips values (taxi): [{min_value}, {max_value}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of shooting data after outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_total_rows(remove_outliers_shooting_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of taxi data after outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_total_rows(remove_outliers_taxi_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename dataframes\n",
    "combined_shooting_dataframe = remove_outliers_shooting_dataframe\n",
    "combined_taxi_dataframe = remove_outliers_taxi_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding to later be combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specify the columns to one-hot encode\n",
    "columns_to_encode = [\"LocationID\", \"Month\", \"Hour\"]\n",
    "\n",
    "# Create a list to store the encoded columns\n",
    "encoded_columns = []\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    # Apply OneHotEncoder to the specified column\n",
    "    encoder = OneHotEncoder(inputCol=column, outputCol=f\"{column}_encoded\")\n",
    "    \n",
    "    # Create a pipeline to apply the transformation\n",
    "    pipeline = Pipeline(stages=[encoder])\n",
    "    combined_taxi_dataframe = pipeline.fit(combined_taxi_dataframe).transform(combined_taxi_dataframe)\n",
    "    \n",
    "    # Add the encoded column to the list\n",
    "    encoded_columns.append(f\"{column}_encoded\")\n",
    "\n",
    "# Select the original columns and the encoded columns\n",
    "selected_columns = [\"LocationID\", \"Date\", \"Weekend\", \"number_of_trips\"] + encoded_columns\n",
    "combined__taxi_dataframe = combined_taxi_dataframe.select(*selected_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for shooting trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specify the columns to one-hot encode\n",
    "columns_to_encode_shooting = [\"LocationID\", \"Month\", \"Hour\"]\n",
    "\n",
    "# Create a list to store the encoded columns\n",
    "encoded_columns_shooting = []\n",
    "\n",
    "for column in columns_to_encode_shooting:\n",
    "    # Apply OneHotEncoder to the specified column\n",
    "    encoder = OneHotEncoder(inputCol=column, outputCol=f\"{column}_encoded\")\n",
    "    \n",
    "    # Create a pipeline to apply the transformation\n",
    "    pipeline = Pipeline(stages=[encoder])\n",
    "    combined_shooting_dataframe = pipeline.fit(combined_shooting_dataframe).transform(combined_shooting_dataframe)\n",
    "    \n",
    "    # Add the encoded column to the list\n",
    "    encoded_columns_shooting.append(f\"{column}_encoded\")\n",
    "\n",
    "# Select the original columns and the encoded columns\n",
    "selected_columns_shooting = [\"LocationID\", \"Date\", \"Weekend\", \"number_of_trips\"] + encoded_columns_shooting\n",
    "combined_shooting_dataframe = combined_shooting_dataframe.select(*selected_columns_shooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to save onehot encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as Parquet file.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a Parquet file\n",
    "output_path = \"../data/curated/onehot_shooting_data.parquet\"\n",
    "combined_shooting_dataframe.write.parquet(output_path, mode=\"overwrite\")\n",
    "\n",
    "print(\"DataFrame saved as Parquet file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as Parquet file.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a Parquet file\n",
    "output_path = \"../data/curated/onehot_taxi_data.parquet\"\n",
    "combined_taxi_dataframe.write.parquet(output_path, mode=\"overwrite\")\n",
    "\n",
    "print(\"DataFrame saved as Parquet file.\")\n",
    "\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
