{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, avg, expr, lit, year\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/20 11:57:09 WARN Utils: Your hostname, LAPTOP-RELH58H1 resolves to a loopback address: 127.0.1.1; using 172.19.22.4 instead (on interface eth0)\n",
      "23/08/20 11:57:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/20 11:57:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1 - Models\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '8g')\n",
    "    .config('spark.executor.memory', '8g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load in processed shooting and taxi data\n",
    "shooting_trips = spark.read.parquet(f'../data/curated/onehot_shooting_data.parquet/*')\n",
    "taxi_trips = spark.read.parquet(f'../data/curated/onehot_taxi_data.parquet/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to count size of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count number of rows in data\n",
    "def calculate_total_rows(dataframe):\n",
    "    \"\"\"\n",
    "    Calculate the total number of rows in a DataFrame\n",
    "    Inputs:\n",
    "        Input DataFrame\n",
    "    Returns:\n",
    "        Total number of rows in the DataFrame\n",
    "    \"\"\"\n",
    "    total_rows = 0\n",
    "    total_rows += dataframe.count()\n",
    "    return total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of aggregated shooting trips: 300549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of aggregated taxi trips: 4240679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use function to count size of dataframes\n",
    "size_of_shooting_data = calculate_total_rows(shooting_trips)\n",
    "size_of_taxi_data = calculate_total_rows(taxi_trips)\n",
    "\n",
    "# Display size of dataframes\n",
    "print(\"Size of aggregated shooting trips:\", size_of_shooting_data)\n",
    "print(\"Size of aggregated taxi trips:\", size_of_taxi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding column to indicate shooting so both pieces of data can be used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add indicator if trip was part of shooting trip dataset, 1 is yes, 0 is no\n",
    "shooting_trips = shooting_trips.withColumn(\"Shooting\", lit(1))\n",
    "taxi_trips = taxi_trips.withColumn(\"Shooting\", lit(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Dataframes to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the common columns to select\n",
    "selected_columns = [\"Date\", \"Weekend\", \"number_of_trips\", \"LocationID_encoded\", \"Month_encoded\", \"Hour_encoded\", \"Shooting\"]\n",
    "    \n",
    "# Select the desired columns from both DataFrames\n",
    "shooting_trips_selected = shooting_trips.select(*selected_columns)\n",
    "taxi_trips_selected = taxi_trips.select(*selected_columns)\n",
    "    \n",
    "# Union the selected DataFrames\n",
    "combined_df = shooting_trips_selected.union(taxi_trips_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of aggregated shooting trips: 4541228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use function to count size of dataframe and display it\n",
    "size_of_training_data = calculate_total_rows(combined_df)\n",
    "print(\"Size of aggregated shooting trips:\", size_of_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dataframes into training (2016-2018) and testing (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date column is of DateType\n",
    "combined_df = combined_df.withColumn('Year', year(combined_df['Date']))\n",
    "\n",
    "# Filter for years 2016, 2017, and 2018\n",
    "df_2016_to_2018 = combined_df.filter((combined_df['Year'] >= 2016) & (combined_df['Year'] <= 2018))\n",
    "# Filter for the year 2019\n",
    "df_2019 = combined_df.filter(combined_df['Year'] == 2019)\n",
    "\n",
    "# Drop the Year column\n",
    "df_2016_to_2018 = df_2016_to_2018.drop('Year')\n",
    "df_2019 = df_2019.drop('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM - It was predicting negative numbers - so made those = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m feature_columns \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mWeekend\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLocationID_encoded\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMonth_encoded\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mHour_encoded\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mShooting\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[39m# Create a VectorAssembler for assembling features\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m assembler \u001b[39m=\u001b[39m VectorAssembler(inputCols\u001b[39m=\u001b[39;49mfeature_columns, outputCol\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfeatures\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Create a GeneralizedLinearRegression model with appropriate parameters for SVR-like regression\u001b[39;00m\n\u001b[1;32m     12\u001b[0m svr_regressor \u001b[39m=\u001b[39m GeneralizedLinearRegression(\n\u001b[1;32m     13\u001b[0m     labelCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumber_of_trips\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     featuresCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     regParam\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m        \u001b[39m# Regularization parameter\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMethod \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m forces keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_kwargs \u001b[39m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/feature.py:5358\u001b[0m, in \u001b[0;36mVectorAssembler.__init__\u001b[0;34m(self, inputCols, outputCol, handleInvalid)\u001b[0m\n\u001b[1;32m   5354\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5355\u001b[0m \u001b[39m__init__(self, \\\\*, inputCols=None, outputCol=None, handleInvalid=\"error\")\u001b[39;00m\n\u001b[1;32m   5356\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5357\u001b[0m \u001b[39msuper\u001b[39m(VectorAssembler, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m-> 5358\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_java_obj(\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.spark.ml.feature.VectorAssembler\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muid)\n\u001b[1;32m   5359\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setDefault(handleInvalid\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5360\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/wrapper.py:84\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m java_obj \u001b[39m=\u001b[39m _jvm()\n\u001b[1;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m java_class\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 84\u001b[0m     java_obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m java_obj(\u001b[39m*\u001b[39mjava_args)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m UserHelpAutoCompletion\u001b[39m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[39mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client\u001b[39m.\u001b[39;49msend_command(\n\u001b[1;32m   1713\u001b[0m     proto\u001b[39m.\u001b[39;49mREFLECTION_COMMAND_NAME \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m     proto\u001b[39m.\u001b[39;49mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_id \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m proto\u001b[39m.\u001b[39;49mEND_COMMAND_PART)\n\u001b[1;32m   1716\u001b[0m \u001b[39mif\u001b[39;00m answer \u001b[39m==\u001b[39m proto\u001b[39m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[39mreturn\u001b[39;00m JavaPackage(name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, jvm_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Load your preprocessed and merged train and test datasets\n",
    "train_data_preprocessed = df_2016_to_2018\n",
    "test_data_preprocessed = df_2019\n",
    "\n",
    "# Define the feature columns\n",
    "feature_columns = [\"Weekend\", \"LocationID_encoded\", \"Month_encoded\", \"Hour_encoded\", \"Shooting\"]\n",
    "\n",
    "# Create a VectorAssembler for assembling features\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Create a GeneralizedLinearRegression model with appropriate parameters for SVR-like regression\n",
    "svr_regressor = GeneralizedLinearRegression(\n",
    "    labelCol=\"number_of_trips\",\n",
    "    featuresCol=\"features\",\n",
    "    family=\"gaussian\",  # Gaussian family for SVR-like regression\n",
    "    link=\"identity\",    # Identity link function\n",
    "    maxIter=50,         # Number of iterations\n",
    "    regParam=0.01        # Regularization parameter\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, svr_regressor])\n",
    "\n",
    "# Train the SVR-like model\n",
    "svr_model = pipeline.fit(train_data_preprocessed)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = svr_model.transform(test_data_preprocessed)\n",
    "\n",
    "# Ensure that predictions are non-negative\n",
    "predictions = predictions.withColumn('prediction', expr('CASE WHEN prediction < 0 THEN 0 ELSE prediction END'))\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"number_of_trips\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r_squared = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Show RMSE and R-squared\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour_encoded: -245.93024554674093\n",
      "Month_encoded: -237.27022209625127\n",
      "Shooting: -234.48369201364176\n",
      "Weekend: 1.7490229654276124\n",
      "LocationID_encoded: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients from the trained model\n",
    "coefficients = svr_model.stages[-1].coefficients\n",
    "\n",
    "# Map coefficients to their corresponding feature names\n",
    "feature_importance = dict(zip(feature_columns, coefficients))\n",
    "\n",
    "# Sort the feature importance dictionary by absolute value of coefficients\n",
    "sorted_feature_importance = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the sorted feature importance\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save SVM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as Parquet file.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a Parquet file\n",
    "output_path = \"../data/curated/SVM_predictions.parquet\"\n",
    "predictions.write.parquet(output_path, mode=\"overwrite\")\n",
    "\n",
    "print(\"DataFrame saved as Parquet file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 11:57:54 WARN Instrumentation: [1f46d46a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/08/20 11:57:57 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/20 11:58:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "23/08/20 11:58:09 WARN Instrumentation: [1f46d46a] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "[Stage 18:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 67.69356292262097\n",
      "R-squared: 0.6419236055575865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load your preprocessed and merged train and test datasets\n",
    "train_data_preprocessed = df_2016_to_2018\n",
    "test_data_preprocessed = df_2019\n",
    "\n",
    "# Define the feature columns\n",
    "feature_columns = [\"Weekend\", \"LocationID_encoded\", \"Month_encoded\", \"Hour_encoded\", \"Shooting\"]\n",
    "\n",
    "# Create a VectorAssembler for assembling features\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Train and evaluate Linear Regression models\n",
    "linear_regressor = LinearRegression(\n",
    "    labelCol=\"number_of_trips\",\n",
    "    featuresCol=\"features\",\n",
    "    regParam=0.0,           # Set regParam to 0 for no regularization\n",
    "    elasticNetParam=0.0,    # Set elasticNetParam to 0 for no regularization\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, linear_regressor])\n",
    "\n",
    "# Train the Linear Regression model\n",
    "linear_model = pipeline.fit(train_data_preprocessed)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = linear_model.transform(test_data_preprocessed)\n",
    "\n",
    "# Ensure that predictions are non-negative\n",
    "predictions = predictions.withColumn('prediction', expr('CASE WHEN prediction < 0 THEN 0 ELSE prediction END'))\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"number_of_trips\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r_squared = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Show RMSE and R-squared\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour_encoded: -247.3826084301114\n",
      "Month_encoded: -238.72067294909752\n",
      "Shooting: -235.93559381640418\n",
      "Weekend: 1.7501939078318434\n",
      "LocationID_encoded: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients from the trained model\n",
    "coefficients = linear_model.stages[-1].coefficients\n",
    "\n",
    "# Map coefficients to their corresponding feature names\n",
    "feature_importance = dict(zip(feature_columns, coefficients))\n",
    "\n",
    "# Sort the feature importance dictionary by absolute value of coefficients\n",
    "sorted_feature_importance = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the sorted feature importance\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as Parquet file.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a Parquet file\n",
    "output_path = \"../data/curated/LinearRegression_predictions.parquet\"\n",
    "predictions.write.parquet(output_path, mode=\"overwrite\")\n",
    "\n",
    "print(\"DataFrame saved as Parquet file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
