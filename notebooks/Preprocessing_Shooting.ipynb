{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, to_date, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/19 19:30:36 WARN Utils: Your hostname, LAPTOP-RELH58H1 resolves to a loopback address: 127.0.1.1; using 172.19.22.4 instead (on interface eth0)\n",
      "23/08/19 19:30:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/19 19:30:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.executor.memory', '2g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert shooting data from csv to parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/mnt/c/Users/61435/Desktop/Applied Data/Project1/mast30034-project-1-HFISHER00/data/landing/shooting_data.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# Step 3: Write the DataFrame to a Parquet file\u001b[39;00m\n\u001b[1;32m      8\u001b[0m parquet_output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/landing/shooting_data.parquet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mparquet(parquet_output_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1656\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/mnt/c/Users/61435/Desktop/Applied Data/Project1/mast30034-project-1-HFISHER00/data/landing/shooting_data.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "# Step 1: Provide the path to the CSV file\n",
    "csv_file_path = \"../data/landing/shooting_data.csv\"\n",
    "\n",
    "# Step 2: Read the CSV file into a Spark DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Write the DataFrame to a Parquet file\n",
    "parquet_output_path = \"../data/landing/shooting_data.parquet\"\n",
    "df.write.parquet(parquet_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of initial shooting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of initial import: 27312\n"
     ]
    }
   ],
   "source": [
    "size = df.count()\n",
    "print(\"Size of initial import:\", size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Features of Initial Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 21\n"
     ]
    }
   ],
   "source": [
    "# Count the number of columns in the DataFrame\n",
    "num_columns = len(df.columns)\n",
    "print(\"Number of columns:\", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unneccesary columns from shooting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/mnt/c/Users/61435/Desktop/Applied Data/Project1/mast30034-project-1-HFISHER00/data/landing/shooting_data_reduced.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m# Step 4: Write the selected DataFrame to a new Parquet file\u001b[39;00m\n\u001b[1;32m     21\u001b[0m parquet_output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/landing/shooting_data_reduced.parquet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m df_selected\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mparquet(parquet_output_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1656\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/mnt/c/Users/61435/Desktop/Applied Data/Project1/mast30034-project-1-HFISHER00/data/landing/shooting_data_reduced.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "# Read the existing Parquet file into a DataFrame\n",
    "parquet_input_path = \"../data/landing/shooting_data.parquet\"\n",
    "df = spark.read.parquet(parquet_input_path)\n",
    "\n",
    "# Select the columns you want to keep and rename them\n",
    "selected_columns = [\n",
    "    \"INCIDENT_KEY as ID\",\n",
    "    \"OCCUR_DATE as Date\",\n",
    "    \"BORO as Borough\",\n",
    "    \"STATISTICAL_MURDER_FLAG as Murder\",\n",
    "    \"PERP_SEX as Perp_Sex\",\n",
    "    \"PERP_RACE as Perp_Race\",\n",
    "    \"VIC_SEX as Vict_Sex\",\n",
    "    \"VIC_RACE as Vict_Race\",    \n",
    "    \"Latitude\",\n",
    "    \"Longitude\"\n",
    "]\n",
    "df_selected = df.selectExpr(*selected_columns)\n",
    "\n",
    "# Write the selected DataFrame to a new Parquet file\n",
    "parquet_output_path = \"../data/landing/shooting_data_reduced.parquet\"\n",
    "df_selected.write.parquet(parquet_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict date between 2016 and 2019. Also note that all instances have valid latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+-----------------+------+--------------------+--------+--------------+--------+--------------+------------------+------------------+\n",
      "|       ID|      Date|      Borough|Jurisdiction_Code|Murder|            Location|Perp_Sex|     Perp_Race|Vict_Sex|     Vict_Race|          Latitude|         Longitude|\n",
      "+---------+----------+-------------+-----------------+------+--------------------+--------+--------------+--------+--------------+------------------+------------------+\n",
      "|165264362|2017-05-27|       QUEENS|                0| false|                null|    null|          null|       M|         BLACK| 40.69855836100004|-73.73531116999999|\n",
      "|151624171|2016-03-28|        BRONX|                0| false|MULTI DWELL - APT...|       F|BLACK HISPANIC|       M|         BLACK| 40.89018055400004|-73.85747035499998|\n",
      "|174579538|2018-02-08|STATEN ISLAND|                0| false|                null|       M|BLACK HISPANIC|       M|         BLACK| 40.64075466000003|-74.07674821299997|\n",
      "|158566071|2016-11-17|     BROOKLYN|                0| false|MULTI DWELL - PUB...|    null|          null|       M|         BLACK|40.669836599000064|-73.91469197199996|\n",
      "|166268793|2017-06-20|     BROOKLYN|                0| false|                null|    null|          null|       M|         BLACK| 40.66305447500008|-73.91097205999995|\n",
      "|153724448|2016-06-05|     BROOKLYN|                0|  true|MULTI DWELL - APT...|    null|          null|       M|         BLACK|40.679669767000064|-73.93613621099996|\n",
      "|152662569|2016-05-01|     BROOKLYN|                2| false|MULTI DWELL - PUB...|    null|          null|       M|         BLACK| 40.63394434800006|-73.91970105199994|\n",
      "|171866986|2017-11-21|     BROOKLYN|                0|  true|MULTI DWELL - APT...|       M|         BLACK|       F|         BLACK| 40.64436740700006|-73.87799278399996|\n",
      "|173914520|2018-01-22|        BRONX|                0| false|                null|    null|          null|       M|BLACK HISPANIC| 40.80719775400007|-73.92429928699994|\n",
      "|201384969|2019-08-20|     BROOKLYN|                0|  true|                null|       M|         BLACK|       M|         BLACK| 40.66473091300003|-73.92117024099997|\n",
      "|178830276|2018-05-01|        BRONX|                0|  true|                null|       M|         BLACK|       M|         BLACK|40.878068416000076|-73.87470609099995|\n",
      "|166413154|2017-06-25|    MANHATTAN|                2| false|MULTI DWELL - PUB...|       M|         BLACK|       M|         BLACK| 40.79199778700007|-73.94971896899995|\n",
      "|204458419|2019-11-02|        BRONX|                0| false|                null|       M|         BLACK|       M|         BLACK| 40.88007341300005|-73.85069555199993|\n",
      "|174579221|2018-02-09|     BROOKLYN|                0| false|                null|    null|          null|       M|         BLACK| 40.65422368000003|-73.91156363399995|\n",
      "|190170259|2018-11-19|        BRONX|                0| false|                null|       M|BLACK HISPANIC|       M|         BLACK| 40.89857311700007|-73.85865463599998|\n",
      "|168461315|2017-08-24|     BROOKLYN|                2| false|MULTI DWELL - PUB...|    null|          null|       M|         BLACK| 40.68354945300007|-73.98849451599995|\n",
      "|156521519|2016-09-07|     BROOKLYN|                0| false|                null|    null|          null|       M|         BLACK| 40.66305517500007|-73.92364037299996|\n",
      "|182363095|2018-05-06|     BROOKLYN|                0| false|                null|    null|          null|       M|         BLACK| 40.68126293200004|-73.91550525499997|\n",
      "|197997725|2019-06-03|     BROOKLYN|                2| false|MULTI DWELL - PUB...|       M|         BLACK|       M|         BLACK| 40.65553981000005|-73.88409743899996|\n",
      "|199247701|2019-07-03|       QUEENS|                2| false|MULTI DWELL - PUB...|       M|         WHITE|       M|WHITE HISPANIC| 40.75544249100005|-73.94342211899993|\n",
      "+---------+----------+-------------+-----------------+------+--------------------+--------+--------------+--------+--------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Provide the path to the Parquet file\n",
    "parquet_file_path = \"../data/landing/shooting_data_reduced.parquet\"\n",
    "\n",
    "# Read the Parquet file into a Spark DataFrame\n",
    "shooting_df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Convert the 'Date' column to a date type using to_date()\n",
    "shooting_df  = shooting_df .withColumn(\"Date\", to_date(col(\"Date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Filter the DataFrame to get shootings between 2016 and 2019\n",
    "shooting_df  = shooting_df .filter(col(\"Date\").between(\"2016-01-01\", \"2019-12-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of restricted shooting data (2016-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of filtered data (2016-2019): 4103\n"
     ]
    }
   ],
   "source": [
    "size = shooting_df.count()\n",
    "print(\"Size of filtered data (2016-2019):\", size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Murder False/True to 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+-----------------+------+--------------------+--------+--------------+--------+--------------+------------------+------------------+\n",
      "|       ID|      Date|      Borough|Jurisdiction_Code|murder|            Location|Perp_Sex|     Perp_Race|Vict_Sex|     Vict_Race|          Latitude|         Longitude|\n",
      "+---------+----------+-------------+-----------------+------+--------------------+--------+--------------+--------+--------------+------------------+------------------+\n",
      "|165264362|2017-05-27|       QUEENS|                0|     0|                null|    null|          null|       M|         BLACK| 40.69855836100004|-73.73531116999999|\n",
      "|151624171|2016-03-28|        BRONX|                0|     0|MULTI DWELL - APT...|       F|BLACK HISPANIC|       M|         BLACK| 40.89018055400004|-73.85747035499998|\n",
      "|174579538|2018-02-08|STATEN ISLAND|                0|     0|                null|       M|BLACK HISPANIC|       M|         BLACK| 40.64075466000003|-74.07674821299997|\n",
      "|158566071|2016-11-17|     BROOKLYN|                0|     0|MULTI DWELL - PUB...|    null|          null|       M|         BLACK|40.669836599000064|-73.91469197199996|\n",
      "|166268793|2017-06-20|     BROOKLYN|                0|     0|                null|    null|          null|       M|         BLACK| 40.66305447500008|-73.91097205999995|\n",
      "|153724448|2016-06-05|     BROOKLYN|                0|     1|MULTI DWELL - APT...|    null|          null|       M|         BLACK|40.679669767000064|-73.93613621099996|\n",
      "|152662569|2016-05-01|     BROOKLYN|                2|     0|MULTI DWELL - PUB...|    null|          null|       M|         BLACK| 40.63394434800006|-73.91970105199994|\n",
      "|171866986|2017-11-21|     BROOKLYN|                0|     1|MULTI DWELL - APT...|       M|         BLACK|       F|         BLACK| 40.64436740700006|-73.87799278399996|\n",
      "|173914520|2018-01-22|        BRONX|                0|     0|                null|    null|          null|       M|BLACK HISPANIC| 40.80719775400007|-73.92429928699994|\n",
      "|201384969|2019-08-20|     BROOKLYN|                0|     1|                null|       M|         BLACK|       M|         BLACK| 40.66473091300003|-73.92117024099997|\n",
      "|178830276|2018-05-01|        BRONX|                0|     1|                null|       M|         BLACK|       M|         BLACK|40.878068416000076|-73.87470609099995|\n",
      "|166413154|2017-06-25|    MANHATTAN|                2|     0|MULTI DWELL - PUB...|       M|         BLACK|       M|         BLACK| 40.79199778700007|-73.94971896899995|\n",
      "|204458419|2019-11-02|        BRONX|                0|     0|                null|       M|         BLACK|       M|         BLACK| 40.88007341300005|-73.85069555199993|\n",
      "|174579221|2018-02-09|     BROOKLYN|                0|     0|                null|    null|          null|       M|         BLACK| 40.65422368000003|-73.91156363399995|\n",
      "|190170259|2018-11-19|        BRONX|                0|     0|                null|       M|BLACK HISPANIC|       M|         BLACK| 40.89857311700007|-73.85865463599998|\n",
      "|168461315|2017-08-24|     BROOKLYN|                2|     0|MULTI DWELL - PUB...|    null|          null|       M|         BLACK| 40.68354945300007|-73.98849451599995|\n",
      "|156521519|2016-09-07|     BROOKLYN|                0|     0|                null|    null|          null|       M|         BLACK| 40.66305517500007|-73.92364037299996|\n",
      "|182363095|2018-05-06|     BROOKLYN|                0|     0|                null|    null|          null|       M|         BLACK| 40.68126293200004|-73.91550525499997|\n",
      "|197997725|2019-06-03|     BROOKLYN|                2|     0|MULTI DWELL - PUB...|       M|         BLACK|       M|         BLACK| 40.65553981000005|-73.88409743899996|\n",
      "|199247701|2019-07-03|       QUEENS|                2|     0|MULTI DWELL - PUB...|       M|         WHITE|       M|WHITE HISPANIC| 40.75544249100005|-73.94342211899993|\n",
      "+---------+----------+-------------+-----------------+------+--------------------+--------+--------------+--------+--------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert \"true\" to 1 and \"false\" to 0 in the \"murder\" column\n",
    "shooting_df_filtered = shooting_df.withColumn(\n",
    "    \"murder\",\n",
    "    when(col(\"murder\") == True, 1)\n",
    "    .when(col(\"murder\") == False, 0)\n",
    "    .otherwise(None)  # Handling any other values that might be present\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "shooting_df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find date for 7 days after shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/mnt/c/Users/61435/Desktop/Applied Data/Project1/mast30034-project-1-HFISHER00/data/raw/shooting_data_2016-2019.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m filtered_parquet_output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/raw/shooting_data_2016-2019.parquet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Write the filtered DataFrame to a new Parquet file\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m shooting_df_filtered\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mparquet(filtered_parquet_output_path)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Stop the SparkSession\u001b[39;00m\n\u001b[1;32m     10\u001b[0m spark\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1656\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/mnt/c/Users/61435/Desktop/Applied Data/Project1/mast30034-project-1-HFISHER00/data/raw/shooting_data_2016-2019.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "shooting_df_filtered = shooting_df_filtered.withColumn(\"Date_7_days_after\", col(\"Date\") + F.expr(\"INTERVAL 7 days\"))\n",
    "\n",
    "# Provide the path to save the filtered DataFrame to a new Parquet file\n",
    "filtered_parquet_output_path = \"../data/raw/shooting_data_2016-2019.parquet\"\n",
    "\n",
    "# Write the filtered DataFrame to a new Parquet file\n",
    "shooting_df_filtered.write.parquet(filtered_parquet_output_path)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
